# OpenAI Multiprocess Inference

A Python package for efficient parallel inference using OpenAI and other LLM APIs (can only be used on .py scripts and not on jupyter notebooks).

## Prerequisites

- Python 3.10+
- Ollama installed: [text](https://ollama.com/download)

## Installation

```bash
pip install git+https://github.com/MediaMonitoringAndAnalysis/llm_multiprocessing_inference.git
```

## Features

- ğŸš€ Parallel inference using asyncio
- ğŸ”„ Support for multiple LLM APIs (OpenAI, Perplexity)
- ğŸ“Š Progress bar for bulk operations
- ğŸ¯ Structured and unstructured response handling
- ğŸ›¡ï¸ Built-in error handling and retries
- âš¡ Optimized for high-throughput scenarios
- ğŸ”„ Support for Ollama

## Quick Start
```python
from llm_multiprocessing_inference import get_answers

# Define your prompts
prompts = [
    [{"role": "user", "content": "What is the capital of France?"}],
    [{"role": "user", "content": "What is the capital of Germany?"}],
]

# Get answers
answers = get_answers(
prompts=prompts,
default_response="{}",
response_type="structured",
api_pipeline="OpenAI",
api_key="your-api-key"
)
```

Here's the complete tree structure for the package:
```plaintext
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â””â”€â”€ src/
    â””â”€â”€ openai_multiproc_inference/
        â”œâ”€â”€ __init__.py
        â””â”€â”€ inference.py
```